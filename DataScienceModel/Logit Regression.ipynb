{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "supervised learning - classification - discrete labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二元分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def logitRegression(data):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    -------\n",
    "    data: DataFrame, 建模数据\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    re = trainModel(train)\n",
    "    modelSummary(re)\n",
    "    \n",
    "def trainModel(data):\n",
    "    formula = \"label_code ~ age + education_num + capital_gain + capital_loss + hours_per_week\"\n",
    "    model = sm.Logit.from_formula(formula, data=data)\n",
    "    re = model.fit()\n",
    "    return re\n",
    "\n",
    "def modelSummary(re):\n",
    "    # 整体统计分析结果\n",
    "    print(re.summary())\n",
    "    \n",
    "    # 用f test检验education_num的系数是否显著\n",
    "    print(\"检验假设education_num的系数等于0\")\n",
    "    print(re.f_test(\"education_test\"))\n",
    "    \n",
    "    # 用f_test检验两个假设是否同时成立\n",
    "    print(\"检验假设education_num的系数等于0.32和hours_per_week的系数等于0.04同时成立\")\n",
    "    print(re.f_test(\"education_num=0.32, hours_per_week=0.04\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 理解模型结果\n",
    "def interpretModel(re):\n",
    "    # re: BinaryResults, 训练好的逻辑回归模型\n",
    "    \n",
    "    # 置信区间\n",
    "    conf = re.conf_int()\n",
    "    conf[\"OR\"] = re.params\n",
    "    # conf里面的3列，分别对应着估计值的下界，上界，和估计值本身\n",
    "    conf.columns = ['2.5%', '97.5%', 'OR']\n",
    "    print(\"各个变量对事件发生的影响：\")\n",
    "    print(np.exp(conf))\n",
    "    \n",
    "    #计算各个变量的边际效应\n",
    "    print(\"各个变量的边际效应\")\n",
    "    print(re.get_margeff(at='overall').summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测结果\n",
    "def makePrediction(re, testSet, alpha=0.5):\n",
    "    testSet['prob'] = re.predict(testSet)\n",
    "    print(\"事件发生概率(预测概率)大于0.6的数据个数:\")\n",
    "    print(testSet[testSet['prob']>0.6].shape[0])\n",
    "    \n",
    "    print(\"事件发生概率(预测概率)大于0.5的数据个数:\")\n",
    "    print(testSet[testSet['prob']>0.5].shape[0])\n",
    "    \n",
    "    # 根据预测的概率，得出最终的预测\n",
    "    testSet['prob'] = testSet.apply(lambda x: 1 if x[\"prob\"] > alpha else 0, axis = 1)\n",
    "    return testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估指标\n",
    "import numpy as np\n",
    "def evaluation(re):\n",
    "    bins = np.array([0, 0.5, 1])\n",
    "    label = re['label_code']\n",
    "    pred = re['pred']\n",
    "    tn, fp, fn, tp = np.histogram2d(label, pred, bins=bins)[0].flatten()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(\"Precision: %3f\\nRecall: %3f\\nF1-score: %3f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC(Receiver Operating Characteristic Cureve) 与 AUC\n",
    "# TPR = TP / (TP + FN)\n",
    "# FPR = FP / (FP + TN)\n",
    "# ROC离左上角越近越好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多元分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def multiLogit(data):\n",
    "    features = ['x1', 'x2']\n",
    "    labels = 'label'\n",
    "    methods = ['multinominal', 'ovr']\n",
    "    \n",
    "    # 使用两种不同的方法对数据建模\n",
    "    for i in range(len(methods)):\n",
    "        model = LogitRegression(multi_class=method[i], \n",
    "                                solver='sag',\n",
    "                                max_iter=1000,\n",
    "                                random_state=42)\n",
    "        model.fit(data[features], data[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多元回归(multinominal) vs One-vs.-all  \n",
    "- 多元回归将原始数据按类别分出一个个数据子集，然后在这些子集的基础上训练模型，因此模型参数的协方差矩阵在这些子集内是恒定的\n",
    "- One-vs.-all的方法则是在个体的基础上建模，参数的协方差矩阵在上面的数据子集内是不恒定的\n",
    "- 多元回归回倾向于使所谓的决策边界(decision boundary)远离原始数据，所以会有更少的点落在两种分类的边界处"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非均衡数据集\n",
    "\n",
    "### 准确度悖论 Accuracy Paradox\n",
    "ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "### 解决方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估指标\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 用权重调整占比较少的类别，使其损失增加\n",
    "def balanceData(X, Y):\n",
    "    positiveWeight = len(Y(Y>0)) / float(len(Y))\n",
    "    classWeight = {1:1./positiveWeight, 0: 1./(1-positiveWeight)}\n",
    "    \n",
    "    # 为了消除惩罚项的干扰，将惩罚系数设为很大\n",
    "    model = LogisticRegression(class_weight=classWeight, C=1e4)\n",
    "    model.fit(X, Y.ravel())\n",
    "    pred = model.predict(X)\n",
    "    return pred\n",
    "\n",
    "# 更多解决方法参考Learning from Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
